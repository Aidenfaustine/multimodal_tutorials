{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-25T21:57:22.869362Z",
     "iopub.status.busy": "2022-01-25T21:57:22.868898Z",
     "iopub.status.idle": "2022-01-25T21:57:24.325750Z",
     "shell.execute_reply": "2022-01-25T21:57:24.325036Z",
     "shell.execute_reply.started": "2022-01-25T21:57:22.869268Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "def one_hot(i):\n",
    "    a=[0 for ii in range(7)]\n",
    "    a[i]=1\n",
    "    return a\n",
    "\n",
    "def get_data_dev():\n",
    "    label_transform={'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
    "    data_dict = {}\n",
    "    csv_path_dev = 'C:/Users/faustineljc/Desktop/mfcc_preprocess/dev_mfcc_d'\n",
    "    csv_files_dev = os.listdir(csv_path_dev)\n",
    "    csv_files_dev.sort(key=lambda x: (int(x.split(\"_\")[0][3:]), int(x.split('_')[1][3:-4])))\n",
    "    for i in range(len(csv_files_dev)):\n",
    "        csv_files_dev[i] = csv_files_dev[i][:-4]\n",
    "    with open('C:/Users/faustineljc/Desktop/melddatasets.yaml', 'rb') as stream:\n",
    "        try:\n",
    "            output = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    for file in csv_files_dev:\n",
    "        for dtt in ['dev', 'test', 'train']:\n",
    "            if file in output[dtt]:\n",
    "                x, y = file, output[dtt][file][\"Emotion\"]\n",
    "                data_dict[x] = one_hot(label_transform[y])\n",
    "                break\n",
    "    tmp_feature = {}\n",
    "    for csv_file in csv_files_dev:\n",
    "        data = pd.read_csv(csv_path_dev + '/' + csv_file + '.csv')\n",
    "        feature = np.array(data, dtype='float32')[:, 2:].tolist()\n",
    "        tmp_feature[csv_file] = feature\n",
    "    return data_dict, tmp_feature\n",
    "\n",
    "def get_data_test():\n",
    "    label_transform = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
    "    data_dict = {}\n",
    "    csv_path_test = 'C:/Users/faustineljc/Desktop/mfcc_preprocess/test_mfcc_d'\n",
    "    csv_files_test = os.listdir(csv_path_test)\n",
    "    csv_files_test.sort(key=lambda x: (int(x.split(\"_\")[0][3:]), int(x.split('_')[1][3:-4])))\n",
    "    for i in range(len(csv_files_test)):\n",
    "        csv_files_test[i] = csv_files_test[i][:-4]\n",
    "    with open('C:/Users/faustineljc/Desktop/melddatasets.yaml', 'rb') as stream:\n",
    "        try:\n",
    "            output = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    for file in csv_files_test:\n",
    "        for dtt in ['dev', 'test', 'train']:\n",
    "            if file in output[dtt]:\n",
    "                x, y = file, output[dtt][file][\"Emotion\"]\n",
    "                data_dict[x] = one_hot(label_transform[y])\n",
    "                break\n",
    "    tmp_feature = {}\n",
    "    for csv_file in csv_files_test:\n",
    "        data = pd.read_csv(csv_path_test + '/' + csv_file + '.csv')\n",
    "        feature = np.array(data, dtype=\"float32\")[:, 2:].tolist()\n",
    "        tmp_feature[csv_file] = feature\n",
    "    return data_dict, tmp_feature\n",
    "\n",
    "def get_data_train():\n",
    "    label_transform = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
    "    data_dict = {}\n",
    "    csv_path_train ='C:/Users/faustineljc/Desktop/mfcc_preprocess/train_mfcc_d'\n",
    "    csv_files_train = os.listdir(csv_path_train)\n",
    "    #print(csv_files_train)\n",
    "    csv_files_train.sort(key=lambda x:(int(x.split(\"_\")[0][3:]), int(x.split('_')[1][3:-4])))\n",
    "    for i in range(len(csv_files_train)):\n",
    "        csv_files_train[i]=csv_files_train[i][:-4]\n",
    "    with open('C:/Users/faustineljc/Desktop/melddatasets.yaml', 'rb') as stream:\n",
    "        try:\n",
    "            # print(yaml.safe_load(stream))\n",
    "            output = yaml.safe_load(stream)\n",
    "            # print(output)\n",
    "            # print(len(output), type(output))  # len 3, type dict\n",
    "            # print(output.keys())  # dict_keys(['dev', 'test', 'train'])\n",
    "            # dev:1108 samples , test:2610 samples , train:9989 samples\n",
    "            # print(len(output[\"dev\"]),len(output[\"test\"]),len(output[\"train\"]))\n",
    "            # print(output[\"dev\"])\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    for file in csv_files_train:\n",
    "        for dtt in ['dev', 'test', 'train']:\n",
    "            if file in output[dtt]:\n",
    "                x, y=file, output[dtt][file][\"Emotion\"]\n",
    "                data_dict[x] = one_hot(label_transform[y])\n",
    "                break\n",
    "    tmp_feature = {}\n",
    "    for csv_file in csv_files_train:\n",
    "        data = pd.read_csv(csv_path_train + '/'+ csv_file + '.csv')\n",
    "        feature = np.array(data, dtype= \"float32\")[:,2:].tolist()\n",
    "        tmp_feature[csv_file]=feature\n",
    "    # audio_feature = torch.tensor(tmp_feature)  # (n,num_time_step,39)\n",
    "    # print(audio_feature.shape)\n",
    "    # #print(audio_feature)\n",
    "    # print(csv_files_train[:15])\n",
    "    return data_dict, tmp_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T21:57:24.327289Z",
     "iopub.status.busy": "2022-01-25T21:57:24.327018Z",
     "iopub.status.idle": "2022-01-25T22:01:42.583558Z",
     "shell.execute_reply": "2022-01-25T22:01:42.582800Z",
     "shell.execute_reply.started": "2022-01-25T21:57:24.327253Z"
    }
   },
   "outputs": [],
   "source": [
    "# it takes times\n",
    "xy_train=get_data_train()  # a tuple (dict, dict)\n",
    "#with open(\"xy_train.txt\", \"w\") as f:\n",
    "#    f.write(xy_train)\n",
    "xy_dev=get_data_dev()\n",
    "#with open(\"xy_dev.txt\", \"w\") as f:\n",
    "#    f.write(xy_dev)\n",
    "xy_test=get_data_test()\n",
    "#with open(\"xy_test.txt\", \"w\") as f:\n",
    "#    f.write(xy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:01:42.585869Z",
     "iopub.status.busy": "2022-01-25T22:01:42.585602Z",
     "iopub.status.idle": "2022-01-25T22:01:42.897114Z",
     "shell.execute_reply": "2022-01-25T22:01:42.896369Z",
     "shell.execute_reply.started": "2022-01-25T22:01:42.585835Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:01:42.898794Z",
     "iopub.status.busy": "2022-01-25T22:01:42.898534Z",
     "iopub.status.idle": "2022-01-25T22:01:42.913679Z",
     "shell.execute_reply": "2022-01-25T22:01:42.913027Z",
     "shell.execute_reply.started": "2022-01-25T22:01:42.898760Z"
    }
   },
   "outputs": [],
   "source": [
    "class Mydata_dev(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xy_dev=xy_dev\n",
    "    def __getitem__(self, index):\n",
    "        file_name_dev = list(self.xy_dev[0].keys())[index]\n",
    "        # data, emotion pair\n",
    "        #nor = (torch.Tensor(self.xy_dev[1][file_name_dev])-torch.mean(torch.Tensor(self.xy_dev[1][file_name_dev]),dim=1,keepdim=True))/torch.std(torch.Tensor(self.xy_dev[1][file_name_dev]),dim=1,keepdim=True)\n",
    "        nor = torch.Tensor(self.xy_dev[1][file_name_dev])\n",
    "        sample_dev = nor, torch.Tensor(self.xy_dev[0][file_name_dev])\n",
    "        return sample_dev\n",
    "    def __len__(self):\n",
    "        return len(self.xy_dev[0])\n",
    "\n",
    "class Mydata_train(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xy_train=xy_train  # a dict\n",
    "    def __getitem__(self, index):\n",
    "        file_name_train=list(self.xy_train[0].keys())[index]\n",
    "        # data, emotion pair\n",
    "        #nor = (torch.Tensor(self.xy_train[1][file_name_train])-torch.mean(torch.Tensor(self.xy_train[1][file_name_train]),dim=1,keepdim=True))/torch.std(torch.Tensor(self.xy_train[1][file_name_train]),dim=1,keepdim=True)\n",
    "        nor = torch.Tensor(self.xy_train[1][file_name_train])\n",
    "        sample_train = nor, torch.Tensor(self.xy_train[0][file_name_train])\n",
    "        return sample_train\n",
    "    def __len__(self):\n",
    "        return len(self.xy_train[0])\n",
    "\n",
    "class Mydata_test(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xy_test=xy_test\n",
    "    def __getitem__(self, index):\n",
    "        file_name_test = list(self.xy_test[0].keys())[index]\n",
    "        # data, emotion pair\n",
    "        #nor = (torch.Tensor(self.xy_test[1][file_name_test])-torch.mean(torch.Tensor(self.xy_test[1][file_name_test]),dim=1,keepdim=True))/torch.std(torch.Tensor(self.xy_test[1][file_name_test]),dim=1,keepdim=True)\n",
    "        nor = torch.Tensor(self.xy_test[1][file_name_test])\n",
    "        sample_test = nor, torch.Tensor(self.xy_test[0][file_name_test])\n",
    "        return sample_test\n",
    "    def __len__(self):\n",
    "        return len(self.xy_test[0])\n",
    "import random\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label.tolist())\n",
    "        text_list.append(_text)\n",
    "    #print(label_list)\n",
    "    label_list = torch.Tensor(label_list)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T20:55:54.723309Z",
     "iopub.status.busy": "2022-01-25T20:55:54.722617Z",
     "iopub.status.idle": "2022-01-25T20:55:54.727399Z",
     "shell.execute_reply": "2022-01-25T20:55:54.726557Z",
     "shell.execute_reply.started": "2022-01-25T20:55:54.723272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_classes = 7\n",
    "num_epochs = 2\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 39\n",
    "sequence_length = 28\n",
    "hidden_size = 128\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T21:38:09.431714Z",
     "iopub.status.busy": "2022-01-25T21:38:09.431284Z",
     "iopub.status.idle": "2022-01-25T21:38:09.440224Z",
     "shell.execute_reply": "2022-01-25T21:38:09.439523Z",
     "shell.execute_reply.started": "2022-01-25T21:38:09.431654Z"
    }
   },
   "outputs": [],
   "source": [
    "d_train=Mydata_train()\n",
    "d_dev=Mydata_dev()\n",
    "d_test=Mydata_test()\n",
    "\n",
    "print(len(d_train))\n",
    "print(len(d_dev))\n",
    "print(len(d_test))\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=d_train,batch_size=batch_size,collate_fn=collate_batch,shuffle=True)\n",
    "dev_loader = DataLoader(dataset=d_dev,batch_size=batch_size,collate_fn=collate_batch,shuffle=True)\n",
    "test_loader = DataLoader(dataset=d_test,batch_size=batch_size,collate_fn=collate_batch,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T15:58:37.215061Z",
     "iopub.status.busy": "2022-01-24T15:58:37.214738Z",
     "iopub.status.idle": "2022-01-24T15:58:37.254965Z",
     "shell.execute_reply": "2022-01-24T15:58:37.253903Z",
     "shell.execute_reply.started": "2022-01-24T15:58:37.215027Z"
    }
   },
   "outputs": [],
   "source": [
    "iterator_train=iter(train_loader)\n",
    "iterator_dev=iter(dev_loader)\n",
    "iterator_test=iter(test_loader)\n",
    "n_train=next(iterator_train)\n",
    "n_dev=next(iterator_dev)\n",
    "n_test=next(iterator_test)\n",
    "print(n_train[0][-1,[0],:], n_train[1])\n",
    "print(n_dev[0].shape, n_dev[1])\n",
    "print(n_test[0].shape, n_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:01:42.915790Z",
     "iopub.status.busy": "2022-01-25T22:01:42.915301Z",
     "iopub.status.idle": "2022-01-25T22:01:42.927029Z",
     "shell.execute_reply": "2022-01-25T22:01:42.926290Z",
     "shell.execute_reply.started": "2022-01-25T22:01:42.915753Z"
    }
   },
   "outputs": [],
   "source": [
    "# single or multiple layer RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "\n",
    "        # or:\n",
    "        # self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        #self.bn = nn.BatchNorm1d(2*hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # or:\n",
    "        # out, _ = self.lstm(x, (h0,c0))\n",
    "\n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:14:04.623468Z",
     "iopub.status.busy": "2022-01-25T22:14:04.623182Z",
     "iopub.status.idle": "2022-01-25T22:14:04.634995Z",
     "shell.execute_reply": "2022-01-25T22:14:04.634032Z",
     "shell.execute_reply.started": "2022-01-25T22:14:04.623430Z"
    }
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        #h0 = torch.normal(0,0.01,(2*self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        #c0 = torch.normal(0,0.01,(2*self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0,c0))\n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "        out = self.bn(out)\n",
    "        out = self.fc(out)\n",
    "        # out: (n, 7)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:14:10.871064Z",
     "iopub.status.busy": "2022-01-25T22:14:10.870784Z",
     "iopub.status.idle": "2022-01-25T22:14:13.978878Z",
     "shell.execute_reply": "2022-01-25T22:14:13.978170Z",
     "shell.execute_reply.started": "2022-01-25T22:14:10.871034Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xy_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d5db404bf6d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0md_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMydata_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0md_dev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMydata_dev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0md_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMydata_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f7b8a8db47a4>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxy_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxy_train\u001b[0m  \u001b[1;31m# a dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mfile_name_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxy_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xy_train' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "# Hyper-parameters\n",
    "num_classes = 7\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 0.0001 #learning_rate = 0.00001\n",
    "\n",
    "input_size = 39\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "d_train=Mydata_train()\n",
    "d_dev=Mydata_dev()\n",
    "d_test=Mydata_test()\n",
    "\n",
    "\n",
    "'''\n",
    "balance data\n",
    "methods for dealing with imbalanced datas： 1.oversampling 2.class weight 3.loss \n",
    "\n",
    "'''\n",
    "#weights = torch.Tensor([4.0, 15.0, 15.0, 3.0, 1.0, 6.0, 3.0])/10\n",
    "#class_weights_train = torch.Tensor([ 2.1208,  8.2896, 37.2724, 14.6252,  5.7309, 36.8598,  9.0072])\n",
    "class_weights_train = torch.Tensor([4.0, 15.0, 15.0, 3.0, 1.0, 6.0, 3.0])\n",
    "sample_weights = [0] * len(d_train)\n",
    "for idx, (data, label) in enumerate(d_train):\n",
    "    class_weight = class_weights_train[torch.argmax(label)]\n",
    "    sample_weights[idx] = class_weight\n",
    "sampler_train=WeightedRandomSampler(sample_weights,num_samples=len(d_train),replacement=True)\n",
    "\n",
    "#class_weights_dev = torch.Tensor([ 2.3596,  7.3933, 27.7250,  9.9910,  6.8037, 50.4091,  7.2484])\n",
    "class_weights_dev = torch.Tensor([4.0, 15.0, 15.0, 3.0, 1.0, 6.0, 3.0])\n",
    "sample_weights = [0] * len(d_dev)\n",
    "for idx, (data, label) in enumerate(d_dev):\n",
    "    class_weight = class_weights_dev[torch.argmax(label)]\n",
    "    sample_weights[idx] = class_weight\n",
    "sampler_dev=WeightedRandomSampler(sample_weights,num_samples=len(d_dev),replacement=True)\n",
    "\n",
    "#class_weights_test = torch.Tensor([ 2.0820,  9.3060, 52.3000, 12.5721,  6.5050, 38.4559,  7.5797])\n",
    "class_weights_test = torch.Tensor([4.0, 15.0, 15.0, 3.0, 1.0, 6.0, 3.0])\n",
    "sample_weights = [0] * len(d_test)\n",
    "for idx, (data, label) in enumerate(d_test):\n",
    "    class_weight = class_weights_test[torch.argmax(label)]\n",
    "    sample_weights[idx] = class_weight\n",
    "sampler_test=WeightedRandomSampler(sample_weights,num_samples=len(d_test),replacement=True)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=d_train,batch_size=batch_size,collate_fn=collate_batch,sampler = sampler_train)\n",
    "dev_loader = DataLoader(dataset=d_dev,batch_size=batch_size,collate_fn=collate_batch,sampler = sampler_dev)\n",
    "test_loader = DataLoader(dataset=d_test,batch_size=batch_size,collate_fn=collate_batch,sampler = sampler_test)\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:14:17.354129Z",
     "iopub.status.busy": "2022-01-25T22:14:17.353553Z",
     "iopub.status.idle": "2022-01-25T22:16:21.524416Z",
     "shell.execute_reply": "2022-01-25T22:16:21.523442Z",
     "shell.execute_reply.started": "2022-01-25T22:14:17.354084Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (mfccdata, labels) in enumerate(train_loader):\n",
    "        # origin shape: [N, NN, 39]\n",
    "        # resized: [N, 28, 28]\n",
    "        # mfccdata = mfccdata.reshape(-1, sequence_length, input_size).to(device)\n",
    "        mfccdata = mfccdata.to(device)\n",
    "        #print(labels)\n",
    "        labels = torch.argmax(labels,dim=1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(mfccdata)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if torch.any(torch.isnan(mfccdata)) or torch.any(torch.isnan(labels)) or torch.any(torch.isnan(outputs)):\n",
    "            continue\n",
    "        #print(labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 100 == 0 or i == 0:\n",
    "            with torch.no_grad():\n",
    "                n_correct = 0\n",
    "                n_samples = 0\n",
    "                for mfccdata, labels in dev_loader:\n",
    "                    mfccdata = mfccdata.to(device)\n",
    "                    labels = torch.argmax(labels,dim=1).to(device)\n",
    "                    outputs = model(mfccdata)\n",
    "                    # max returns (value ,index)\n",
    "                    predicted = torch.argmax(outputs.data, 1).flatten()\n",
    "                    #print(\"predicted:\",predicted)\n",
    "                    #print(\"labels   :\",labels)\n",
    "                    n_samples += len(labels)\n",
    "                    n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                acc = n_correct / n_samples\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{n_total_steps}], Loss: {loss.item():.4f}, Val_acc: {acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T22:16:25.130761Z",
     "iopub.status.busy": "2022-01-25T22:16:25.130514Z",
     "iopub.status.idle": "2022-01-25T22:16:29.600754Z",
     "shell.execute_reply": "2022-01-25T22:16:29.599955Z",
     "shell.execute_reply.started": "2022-01-25T22:16:25.130732Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for mfccdata, labels in test_loader:\n",
    "        mfccdata = mfccdata.to(device)\n",
    "        labels = torch.argmax(labels,dim=1).to(device)\n",
    "        outputs = model(mfccdata)\n",
    "        predicted = torch.argmax(outputs.data, 1).flatten()\n",
    "        #print(\"predicted:\",predicted)\n",
    "        #print(\"labels   :\",labels)\n",
    "        n_samples += len(labels)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods for dealing with imbalanced datasets: \n",
    "#weighted random sampler \n",
    "#focal loss 不平衡文本分类，focal loss理论及pytorch实现 https://blog.51cto.com/u_15127571/3869386 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
